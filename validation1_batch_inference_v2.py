import json
import torch
import os
import argparse
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# ================= 1. é’ˆå¯¹ MI50 çš„ç¯å¢ƒé…ç½® =================
# MI50 (gfx906) å¿…å¤‡
os.environ["HSA_OVERRIDE_GFX_VERSION"] = "9.0.6"
# è§£å†³ç¢ç‰‡åŒ–æ˜¾å­˜åˆ†é…å¤±è´¥çš„é—®é¢˜
os.environ["PYTORCH_HIP_ALLOC_CONF"] = "expandable_segments:True"


TRAIN_PROMPT = """
è¯·åŸºäºä»¥ä¸‹æ–°é—»æŠ¥é“ï¼Œæå–å¹¶ç»“æ„åŒ–å…³é”®ä¿¡æ¯ï¼Œä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹è¦æ±‚è¾“å‡ºï¼š
EVENT_TITLEï¼šç”¨ä¸€å¥ç®€æ´çš„ä¸­æ–‡æ¦‚æ‹¬æ ¸å¿ƒäº‹ä»¶ï¼Œé¿å…ç›´æ¥å¤åˆ¶åŸæ ‡é¢˜ã€‚
EVENT_BRIEFï¼šç”¨1-2å¥ä¸­æ–‡æç‚¼äº‹ä»¶æœ€æ ¸å¿ƒçš„è¦ç´ ï¼ˆäººç‰©ã€åœ°ç‚¹ã€æ ¸å¿ƒå†²çª/é£é™©ï¼‰ã€‚
EVENT_TEXTï¼šç”¨ä¸€ä¸ªè¿è´¯çš„ä¸­æ–‡æ®µè½ï¼ˆçº¦3-5å¥ï¼‰è¯¦ç»†æè¿°äº‹ä»¶èƒŒæ™¯ã€ç»è¿‡ã€å„æ–¹ç«‹åœºå’Œç°çŠ¶ã€‚æ•´åˆ>åŸæ–‡å…³é”®ç»†èŠ‚ï¼Œç¡®ä¿ä¿¡æ¯å®Œæ•´ã€‚
RATEï¼šå¯¹äº‹ä»¶åœ¨ä»¥ä¸‹ç»´åº¦çš„æ½œåœ¨æˆ–å®é™…å½±å“è¿›è¡Œ0-10åˆ†çš„è¯„åˆ†ï¼ˆ0ä¸ºæ— å½±å“ï¼Œ10ä¸ºæå¤§å½±å“ï¼‰ï¼š
å›½å®¶æ”¿ç­–ï¼šä¾æ®æ”¿ç­–å½±å“çš„å¹¿åº¦ä¸æ·±åº¦è¯„åˆ†ï¼Œä»>åœ°æ–¹æ€§æªæ–½åˆ°é‡å¤§å›½ç­–ã€‚
å›½é™…å…³ç³»ï¼šä¾æ®äº‹ä»¶å¯¹å›½é™…æˆ–åœ°åŒºå±€åŠ¿çš„æ”¹å˜ç¨‹åº¦è¯„åˆ†ï¼Œä»æ—¥å¸¸æ´»åŠ¨åˆ°æˆ˜äº‰å†²çªã€‚
æ”¿æ²»å½±å“ï¼šä¾æ®äº‹ä»¶çš„æ”¿æ²»æ•æ„Ÿæ€§ä¸å±‚çº§è¯„åˆ†ï¼Œä»æ—¥å¸¸æ´»åŠ¨åˆ°æœ€é«˜å±‚é‡å¤§å˜æ•…ã€‚
å•†ä¸šé‡‘èï¼šä¾æ®å¯¹ç»æµé‡‘èä½“ç³»çš„å†²å‡»ç¨‹åº¦è¯„åˆ†ï¼Œä»å…¬å¸åŠ¨å‘åˆ°ç³»ç»Ÿå±æœºã€‚
ç§‘æŠ€ä¿¡æ¯ï¼šä¾æ®æŠ€æœ¯çš„çªç ´æ€§ä¸å½±å“åŠ›è¯„åˆ†ï¼Œä»ä¸€èˆ¬æŠ¥é“åˆ°é¢ è¦†æ€§çªç ´ã€‚
ç¤¾ä¼šäº‹ä»¶ï¼šä¸»è¦ä¾æ®äº‹ä»¶æ¶æ€§ç¨‹åº¦ä¸å‘ç”Ÿåœ°ï¼ˆä¸­å›½å›½å†…äº‹ä»¶è¯„åˆ†æ˜¾è‘—é«˜äºå›½å¤–åŒç±»äº‹ä»¶ï¼‰è¯„åˆ†ã€‚
å…¶å®ƒä¿¡æ¯ï¼šç”¨äºå½’ç±»ä¸Šè¿°å…­ç±»ä¹‹å¤–çš„ä¿¡æ¯ï¼Œå¹¶æ ¹æ®å…¶ä»·å€¼ç»™äºˆ0-8åˆ†ã€‚
å†…å®¹å‡†ç¡®ç‡ï¼šåŸºäºåŸæ–‡>ä¿¡æ¯æ˜ç¡®æ€§å’Œæ¥æºå¯ä¿¡åº¦è¯„åˆ†ã€‚
"""


def main(adapter_path, base_model_path, test_data_path, output_file):
    print(f"ğŸ”„ Processing Adapter: {adapter_path}")
    print(f"ğŸ¤– Base Model: {base_model_path}")
    print(f"ğŸ“‚ Input Data: {test_data_path}")
    print(f"ğŸ’¾ Output File: {output_file}")

    # ================= 2. åŠ è½½æ•°æ®ä¸æ–­ç‚¹ç»­ä¼ æ£€æµ‹ =================
    with open(test_data_path, 'r', encoding='utf-8') as f:
        all_data = json.load(f)

    start_index = 0
    if os.path.exists(output_file):
        with open(output_file, 'r', encoding='utf-8') as f_out:
            lines = [line for line in f_out if line.strip()]
            start_index = len(lines)
            if start_index > 0:
                print(f"âš ï¸  Found existing file with {start_index} samples. Resuming...")

    data_to_process = all_data[start_index:]
    if len(data_to_process) == 0:
        print("ğŸ‰ All data processed! Nothing to do.")
        return

    # ================= 3. åŠ è½½æ¨¡å‹ (æ”¹ä¸º FP16 ä»¥é€‚åº” MI50) =================
    print("â³ Loading model into VRAM (FP16 Mode)...")

    tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)

    # ã€å…³é”®ä¿®æ”¹ã€‘ä½¿ç”¨ float16ã€‚
    # 14B FP16 ~28GB VRAMã€‚å•å¼  MI50 (32G) å‹‰å¼ºèƒ½æ”¾ä¸‹ï¼Œ
    # ä½†å»ºè®®ä½¿ç”¨ device_map="auto" è®©ä¸¤å¼ å¡åˆ†æ‹…ï¼Œç•™å‡ºç©ºé—´ç»™æ¨ç†ä¸Šä¸‹æ–‡ã€‚
    model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        device_map="auto",
        torch_dtype=torch.float16,
        trust_remote_code=True
    )

    print(f"ğŸ”— Merging LoRA weights from {adapter_path}...")
    model = PeftModel.from_pretrained(model, adapter_path)
    model.eval()

    # ================= 4. æµå¼æ¨ç†ä¸å†™å…¥ =================
    print(f"ğŸš€ Starting inference on remaining {len(data_to_process)} samples...")

    os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else ".", exist_ok=True)

    with open(output_file, 'a', encoding='utf-8', buffering=1) as f:
        for item in tqdm(data_to_process, desc="Inference"):
            # instruction = item.get("instruction", "")
            instruction = TRAIN_PROMPT              # ä½¿ç”¨è®­ç»ƒæ—¶çš„prompt
            input_text = item.get("input", "")
            # ground_truth = item.get("output", "") # æ¨ç†æ—¶å…¶å®ä¸éœ€è¦ GT

            # ã€å…³é”®ä¿®æ”¹ã€‘DeepSeek-R1 æ¨¡æ¿é€‚é…
            # å¾ˆå¤š R1 Distill æ¨¡å‹ä¸éœ€è¦å¼ºåˆ¶ System Promptï¼Œæˆ–è€…ä¾é  tokenizer_config.json è‡ªåŠ¨å¤„ç†
            # è¿™é‡Œæ„å»ºæ ‡å‡† chat æ ¼å¼ï¼Œè®© tokenizer è‡ªå·±å»æ‹¼å‡‘ <|im_start|> ç­‰æ ‡è®°
            messages = [
                {"role": "user", "content": instruction + "\n" + input_text}
            ]

            # å¦‚æœä½ ç¡®å®è§‰å¾—éœ€è¦ system promptï¼Œå¯ä»¥åœ¨ä¸Šé¢åŠ ï¼Œä½† R1 ç»å¸¸ä¼šå¿½ç•¥å®ƒè€Œç›´æ¥å¼€å§‹ <think>

            text = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )

            model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

            try:
                with torch.no_grad():
                    generated_ids = model.generate(
                        **model_inputs,
                        max_new_tokens=8192,  # R1 æ¨¡å‹é€šå¸¸è¯æ¯”è¾ƒå¤šï¼ˆåŒ…å«æ€è€ƒè¿‡ç¨‹ï¼‰ï¼Œå»ºè®®è°ƒå¤§
                        temperature=0.6,  # R1 å»ºè®®æ¸©åº¦ç¨ä½ä¸€ç‚¹ï¼Œæˆ–è€… 0.6-0.7
                        top_p=0.9,
                        eos_token_id=tokenizer.eos_token_id
                    )

                # åªæˆªå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†
                generated_ids = [
                    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
                ]
                response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]

                # å¯é€‰ï¼šå¦‚æœæ¨¡å‹è¾“å‡ºäº† <think> æ ‡ç­¾ï¼Œä½ å¯èƒ½æƒ³åœ¨è¿™é‡Œåšäº›åå¤„ç†

            except Exception as e:
                print(f"\nâŒ Error generating sample: {e}")
                response = f"ERROR_GENERATION: {str(e)}"

            result_entry = {
                "instruction": instruction,
                "input": input_text,
                "model_output": response,
                "adapter": adapter_path
            }

            f.write(json.dumps(result_entry, ensure_ascii=False) + "\n")
            f.flush()

    print(f"âœ… Done! Results saved to {output_file}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    # å¢åŠ  base_model å‚æ•°ï¼Œä¸å†ç¡¬ç¼–ç 
    parser.add_argument("--base_model", type=str, required=True,
                        help="Path to Base Model (DeepSeek-R1-Distill-Qwen-14B)")
    parser.add_argument("--adapter", type=str, required=True, help="Path to LoRA checkpoint folder")
    parser.add_argument("--data", type=str, default="alpaca_test.json", help="Path to test json")
    parser.add_argument("--output", type=str, required=True, help="Path to output jsonl")

    args = parser.parse_args()
    main(args.adapter, args.base_model, args.data, args.output)

"""
python validation1_batch_inference_v2.py \
--base_model /home/sleepy/Depot/ModelTrain/qwen/DeepSeek-R1-Distill-Qwen-14B \
--adapter /home/sleepy/Depot/ModelTrain/qwen/DeepSeek-R1-LoRA/checkpoint-700 \
--data /home/sleepy/Depot/ModelTrain/IntelligenceIntegrationSystemModelTraining/Data/v1/alpaca_test.json \
--output result_checkpoint-700.jsonl
"""

