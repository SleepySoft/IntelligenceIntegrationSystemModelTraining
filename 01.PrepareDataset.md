概念与设计
====

我通过以下prompt让AI编写提取、清洗及生成数据集的代码，阅读该prompt就能了解数据集的设计思想以及数据集的格式：

```text
我想进行大语言模型微调，请帮我编写python程序一步一步地进行数据提取和预处理，每步只依赖上一步的结果，每个步骤相对独立。

数据存储在mongodb仓库IntelligenceIntegrationSystem中，其中包含两个collection：
1. intelligence_cached - 未处理的原始/缓存数据，遵循CollectedData定义。
2. intelligence_archived - 已处理的数据/归档数据，遵循ArchivedData定义。

数据定义如下，请记住并充分理解。下面我的说明将围绕着这个定义进行：

class CollectedData(BaseModel):
    UUID: str = Field(..., min_length=1)    # [MUST]: The UUID to identify a message.
    token: str = Field(..., min_length=1)   # [MUST]: The token to identify the legal end point.
    source: str | None = None               # (Optional): Message source. If it requires reply.
    target: str | None = None               # (Optional): Message source. If it requires reply.
    prompt: str | None = None               # (Optional): The prompt to ask LLM to process this message.

    title: str | None = None                # [MUST]: The content to be processed.
    authors: List[str] | None = []          # (Optional): Article authors.
    content: str                            # [MUST]: The content to be processed.
    pub_time: object | None = None          # (Optional): Content publish time. Can be time.struct_time, datetime, str, ...
    informant: str | None = None            # (Optional): The source of message (like URL).


class ProcessedData(BaseModel):
    UUID: str = Field(..., min_length=1)
    INFORMANT: str = Field(..., min_length=1)
    PUB_TIME: str | datetime.datetime | None = None

    TIME: list | None = Field(default_factory=list)
    LOCATION: list | None = Field(default_factory=list)
    PEOPLE: list | None = Field(default_factory=list)
    ORGANIZATION: list | None = Field(default_factory=list)
    EVENT_TITLE: str | None = Field(..., min_length=1)
    EVENT_BRIEF: str | None = Field(..., min_length=1)
    EVENT_TEXT: str | None = None

    RATE: dict | None = {}
    IMPACT: str | None = None
    TIPS: str | None = None


class ArchivedDataExtraFields(BaseModel):
    RAW_DATA: dict | None = None
    SUBMITTER: str | None = None
    APPENDIX: dict | None = None


class ArchivedData(ProcessedData, ArchivedDataExtraFields):
    pass


APPENDIX_TIME_GOT           = '__TIME_GOT__'            # Timestamp of get from collector
APPENDIX_TIME_POST          = '__TIME_POST__'           # Timestamp of post to processor
APPENDIX_TIME_DONE          = '__TIME_DONE__'           # Timestamp of retrieve from processor
APPENDIX_TIME_ARCHIVED      = '__TIME_ARCHIVED__'
APPENDIX_RETRY_COUNT        = '__RETRY_COUNT__'
APPENDIX_ARCHIVED_FLAG      = '__ARCHIVED__'
APPENDIX_MAX_RATE_CLASS     = '__MAX_RATE_CLASS__'
APPENDIX_MAX_RATE_SCORE     = '__MAX_RATE_SCORE__'
APPENDIX_MAX_RATE_CLASS_EXCLUDE = '内容准确率'

APPENDIX_MANUAL_RATING      = '__MANUAL_RATING__'

APPENDIX_AI_SERVICE         = '__AI_SERVICE__'
APPENDIX_AI_MODEL           = '__AI_MODEL__'

APPENDIX_LINK_UPSTREAM      = '__LINK_UPSTREAM__'
APPENDIX_LINK_DOWNSTREAM    = '__LINK_DOWNSTREAM__'


ARCHIVED_FLAG_DROP = 'D'
ARCHIVED_FLAG_ERROR = 'E'
ARCHIVED_FLAG_RETRY = 'R'
ARCHIVED_FLAG_ARCHIVED = 'A'
ARCHIVED_FLAG_SENSITIVE = 'S'

第一步：
    * 数据抽取和基本清洗
        + 最后结果分别保存到json文件，utf-8，易读格式

    * 列出所有清洗后丢弃的数据的摘要：
        + 从intelligence_cached中找到 f"APPENDIX.{APPENDIX_ARCHIVED_FLAG}" 为 f"{ARCHIVED_FLAG_DROP}"标记的项。
        + 记录以下字段（不需要按照原有格式）： UUID、pub_time、informant。
        + 去除UUID及informant有重复的项，去除informant不是url的项，并给出报告。

    * 列出所有清洗后归档数据的摘要：
        + 从intelligence_archived中提取数据，记录以下字段（不需要按照原有格式）：
            + UUID
            + INFORMANT
            + f"APPENDIX.{APPENDIX_TIME_ARCHIVED}"
            + f"APPENDIX.{APPENDIX_MAX_RATE_SCORE}"
        + 去除以下项，并给出报告：
            + UUID及INFORMANT重复
            + UUID必须同时在intelligence_cached中存在，且 f"APPENDIX.{APPENDIX_ARCHIVED_FLAG}" 必须为 f"{ARCHIVED_FLAG_ARCHIVED}"
            + INFORMANT不是url的项
            + EVENT_TITLE，EVENT_BRIEF，EVENT_TEXT主体必须是中文（使用简单方法判定，不要使用重型AI）

第二步：
    * 训练及测试、验证数据采样：
        + 指定需要的训练数据条数，计算出额外需要的测试及验证集数据量
        + 根据以上分析，给出丢弃数据及归档数据中采样的比例的建议（也可以由用户指定）。
        + 输出数据采样详细统计信息。
        + 采样完成后分训练集、测试集、验证集保存为三个json文件，理论上只需要保存UUID。

    * 在丢弃数据中采样：
        + 按前一步确定的数量，综合考虑informant和pub_time，使这二者的项均匀分布

    * 在归档数据中采样：
        + 按前一步确定的数量，综合考虑INFORMANT，APPENDIX_TIME_ARCHIVED，APPENDIX_MAX_RATE_SCORE，使这几项合理分布（不一定均匀，我也不确定，请给出建议）。

第三步：
    生成训练数据（Alpaca格式）
        + 仔细阅读并理解以下要求，提取数据并组织为训练数据，需要显示处理进度。
        + 最后同样分训练集、测试集、验证集保存。
        + 同样给出结果详细摘要。

    输入数据（原始数据）：
        + 根据上一步的采样结果，通过UUID从intelligence_cached中获取原始数据。
        + 数据原本的分析是通过以下函数构建对话的，请参考或使用它，尽量按对话时同样的格式构建训练数据。
        + 其中prompt请仔细思考，设计一个简单有效，且具有一定泛用性的文本。如果我没理解错，这段文本将作为该模型今后的“功能开关”。
        + 特殊要求：评分减1（不得小于0），如果为0则归为丢弃数据一类。
        ```
        def build_analyze_message(
                prompt: str,
                structured_data: Dict[str, Any],
                context: Optional[List[Dict[str, str]]] = None):
            try:
                sanitized_data = AIMessage.model_validate(structured_data).model_dump(exclude_unset=True, exclude_none=True)
            except ValidationError as e:
                logger.error(f'AI require data field missing: {str(e)}')
                return {'error': str(e)}
            except Exception as e:
                logger.error(f'Validate AI data fail: {str(e)}')
                return {'error': str(e)}

            metadata_items = [f"- {k}: {v}" for k, v in sanitized_data.items() if k != "content"]
            metadata_block = '## metadata\n' + "\n".join(metadata_items)
            content_block = f"\n\n## 正文内容\n{sanitized_data['content']}"
            user_message = metadata_block + content_block

            messages = context if context else []
            messages.append({"role": "system", "content": prompt})
            messages.append({"role": "user", "content": user_message})

            return messages
        ```

    输出数据：
        对于被判定为需要丢弃的数据，输出结果为：
            ````{"UUID": "输入的UUID原值"}````

        对于非丢弃的数据，输出结果为：
            ProcessedData格式，对应数据从intelligence_archived中提取。

为了使输出不混乱且可验证，我们先分析并执行第一步。
```


操作与使用
====

## TIPS

**Data目录下有我生成的数据集，可以直接用来进行训练。**

## 原始数据

首先，MongoDB数据库中需要有数据。离线数据可以通过以下链接获取：

[不定期手工导出](https://pan.baidu.com/s/122mewzpNkd6A8UjMDpIMsg?pwd=tfx7)

可以运行以下命令导入（高版本需外额外下载mongodb tools）：

```bash
mongoimport --uri=mongodb://localhost:27017 --db=IntelligenceIntegrationSystem --collection=intelligence_cached --file=intelligence_cached.json
mongoimport --uri=mongodb://localhost:27017 --db=IntelligenceIntegrationSystem --collection=intelligence_archived --file=intelligence_archived.json
```

**注意：archived和cache数据都需要导入，因为原始数据统一从cache中获取。**

## 生成训练及测试数据

以上prompt的三个步骤分别对应以下三个python脚本，需要生成数据集时，依次运行这三个文件即可。

最终生成的"alpaca_train.json"及"alpaca_test.json"均为alpaca格式，可直接用于接下来的训练和结果验证。

+ [step1_extract_clean.py](step1_extract_clean.py)

+ [step2_sampling_split.py](step2_sampling_split.py)
    > 可配置参数：
    >+ TARGET_TOTAL_COUNT : 提取数据的总量
    >+ EXPECTED_RATIO_DROPPED - 其中包含丢弃数据（评价为无价值数据）的比例
    >+ SPLIT_RATIOS - 训练集、测试集、验证集的划分比例（验证集其实不需要）

+ [step3_generate_alpaca.py](step3_generate_alpaca.py)
    > 可配置参数：
    >+ PREVIEW_LIMIT : 生成预览文件包含的数据集量，0或None则直接生成全量数据集文件。
    >+ SYSTEM_PROMPT : 训练时输入的prompt，相对于prompt，这句话更像一个功能开关，用来触发训练时学习到的分析模式。

**注意：这三个文件前一个的输出为后一个的输入，不建议修改输入输出文件名。**
