概念与设计
====

## Overview

经过综合考虑，我们最终决定使用 LLaMA-Factory 通过 LoRA 的方式对小模型进行微调。基座模型选择 Qwen2.5-7B-Instruct 。

我的服务器操作系统为Ubuntu 2204，计算卡为 2 x AMD MI 50 32G。

由于该计算卡的特殊性，调教过程非常折腾。如果你不幸也是该卡用户，请参考这篇文章：[FinetuneRecord_20251214.md](FinetuneRecord_20251214.md)

本文仅描述通用的配置部分。


## 超参数设计

### 训练数据量

5600条（7000 * 0.8），其中包含50%的被判断为无价值的数据。

### LoRA 核心参数

| 参数名                  | 设定值      | 解释与设定理由                                                                                                       |
| -------------------- | -------- | ------------------------------------------------------------------------------------------------------------- |
| \--learning_rate     | 1.00E-05 | 解释: 学习率。理由: 这是一个保守且安全的值。通常 LoRA 可以开到 1e-4，但为了防止 Loss 波动或数值溢出（特别是在 FP16/FP32 切换时），1e-5 配合 Cosine 调度器能保证最稳健的收敛。 |
| \--lr_scheduler_type | cosine   | 解释: 余弦退火调度器。理由: 先预热（Warmup）后缓慢下降，在训练后期能让模型更好地收敛到最优解，优于线性的 linear。                                             |
| \--max_grad_norm     | 0.5      | 解释: 梯度裁剪阈值。理由: 强力限制梯度的最大范数。这是防止梯度爆炸 (Gradient Explosion) 和 NaN 错误的最重要防线之一。                                    |

### 训练与优化参数

| 参数名                | 设定值  | 解释与设定理由                                                                                                                          |  |  |  |  |
| ------------------ | ---- | -------------------------------------------------------------------------------------------------------------------------------- |  |  |  |  |
| \--finetuning_type | lora | 解释: 仅训练低秩适配器，冻结原模型权重。理由: 7B 全量参数微调消耗极大，LoRA 能以极小的显存代价达到接近全量微调的效果。                                                                |  |  |  |  |
| \--lora_rank       | 64   | 解释: LoRA 矩阵的秩（Rank），决定了可训练参数的“容量”。理由: 相比通常的 8 或 16，设定为 64 是为了增强领域适应能力。我们希望模型不仅学习回复格式，还能学习特定领域（如智库、特定业务逻辑）的知识，高 Rank 能提供更多信息存储空间。 |  |  |  |  |
| \--lora_alpha      | 128  | 解释: LoRA 更新的缩放系数。公式为 ΔW×rα​。理由: 经验法则是 Alpha = 2 \* Rank。这样可以确保在初始化时，梯度的更新幅度足够大，有助于模型快速收敛。                                        |  |  |  |  |
| \--lora_target     | all  | 解释: 将 LoRA 挂载到所有线性层（q,k,v,o,gate,up,down）。理由: 早期 LoRA 仅微调 Attention (q,v)，但研究表明全模块微调 (Full LoRA) 在逻辑推理和复杂指令遵循任务上效果显著更好。          |  |  |  |  |
| \--lora_dropout    | 0.05 | 解释: 随机丢弃 5% 的神经元连接。理由: 防止过拟合的标准手段，增加模型的泛化能力。                                                                                     |  |  |  |  |


### 显存与批处理参数 (空间换时间，仅供参考)

| 参数名                            | 设定值  | 解释与设定理由                                                                                                       |
| ------------------------------ | ---- | ------------------------------------------------------------------------------------------------------------- |
| \--per_device_train_batch_size | 1    | 解释: 每个 GPU 单次前向传播处理的样本数。理由: 为了给 FP32 全精度权重（约 28GB）留出空间，物理 Batch 必须压到极限。虽然慢，但保证了不爆显存。                          |
| \--gradient_accumulation_steps | 32   | 解释: 梯度累积步数。理由: 模拟大 Batch。计算公式：Total=1(Batch)×2(GPUs)×32(Accum)=64。Batch Size = 64 是指令微调的一个黄金标准，能保证梯度更新方向的准确性。 |
| \--gradient_checkpointing      | TRUE | 解释: 梯度检查点技术。理由: 牺牲 30% 计算速度换取 50% 显存。这是在单卡显存有限（32GB）时强行训练 7B 模型的必要手段。                                         |


### 精度与稳定性参数（仅针对于我的环境）

| 参数名              | 设定值      | 解释与设定理由                                                                                                                  |
| ---------------- | -------- | ------------------------------------------------------------------------------------------------------------------------ |
| \--fp16 / --bf16 | FALSE    | 解释: 关闭混合精度，强制使用 FP32。理由: 针对旧架构或特定环境（如 Vega20）的终极稳定方案。虽然 FP16 更快，但容易溢出（Loss=0/NaN）；FP32 虽然慢且占显存，但数值范围极大，确保训练绝对不会因为精度问题崩溃。 |
| \--flash_attn    | disabled | 解释: 禁用 Flash Attention 加速库。理由: 兼容性考量。在非最新 NVIDIA 架构上，原生 Attention 虽然慢，但通用性最好，不会出现算子报错。                                   |


操作与使用
====

## 安装系统级依赖 (Ubuntu APT)

```bash
sudo apt update && sudo apt upgrade -y
sudo apt install -y git wget curl build-essential libopenmpi-dev
```

## 准备 Python 环境 (Conda)

```bash
# 1. 创建虚拟环境
conda create -n iis_finetune python=3.10 -y

# 2. 激活环境
conda activate iis_finetune
```

## 安装  LLaMA-Factory 框架

```bash
# 1. 拉取源码
cd /home/sleepy/Depot/ModelTrain  # 你的工作目录
git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory

# 2. 安装依赖
# [metrics] 选项会安装评估所需的额外库（如 nltk, jieba, rouge-chinese）
pip install -e ".[metrics]"

# 3. 验证安装
llamafactory-cli version
```

## 可能需要手工安装的依赖

+ PyTorch: 根据硬件选择对应的 CUDA 或 ROCm 版本。

+ Transformers: 建议 >= 4.45.0 以完美支持 Qwen2.5。

+ DeepSpeed (可选): 如果后续需要 ZeRO 优化，需独立安装。

+ Bitsandbytes (可选): 如果需要 4-bit/8-bit 量化训练（QLoRA），需安装。

## 下载基础模型

### 方法一：Hugging Face (国际)

```bash
pip install -U "huggingface_hub[cli]"

huggingface-cli download Qwen/Qwen2.5-7B-Instruct \
    --local-dir /home/sleepy/Depot/ModelTrain/qwen/Qwen2___5-7B-Instruct \
    --resume-download
```

### 方法二：ModelScope (国内)

```bash
pip install modelscope

python -c "from modelscope import snapshot_download; snapshot_download('qwen/Qwen2.5-7B-Instruct', cache_dir='.', revision='master')"
```

## 训练脚本（参数解释见上面，其中有一部分和具体环境相关）

[train_qwen.sh](TrainingScripts/train_qwen.sh)

## 启动训练

+ 将train_qwen.sh放入 LLaMA-Factory 目录中。

+ 将训练数据放入 LLaMA-Factory/data 目录中。

+ 检查并核对train_qwen.sh中的参数后，在iis_finetune环境下运行该脚本。
```bash
conda activate iis_finetune

# 方法1：直接启动
./train_qwen.sh

# 方法2：将Log同时输出到屏幕和文件
./train_qwen.sh 2>&1 | tee train_log.txt

# 注意：如果需要关闭终端，按 Ctrl+Z，然后输入 bg 放入后台，再输入 disown -h 脱离终端关联。
#      或者直接使用nohup启动脚本。
```
