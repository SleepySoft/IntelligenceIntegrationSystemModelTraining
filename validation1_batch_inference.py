import json
import torch
import os
import argparse
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel


"""
# æŒ‡å®šä½¿ç”¨ GPU 0
export HIP_VISIBLE_DEVICES=0

python validation1_batch_inference.py \
    --adapter ./saves/qwen2.5-7b-intelligence/lora/sft_ddp_fp32/checkpoint-100 \
    --data data/alpaca_test.json \
    --output result_ckpt100.jsonl

# æŒ‡å®šä½¿ç”¨ GPU 1
export HIP_VISIBLE_DEVICES=1

python validation1_batch_inference.py \
    --adapter ./saves/qwen2.5-7b-intelligence/lora/sft_ddp_fp32/checkpoint-150 \
    --data data/alpaca_test.json \
    --output result_ckpt150.jsonl
"""


# ================= 1. é’ˆå¯¹ MI50 çš„ç¯å¢ƒé…ç½® =================
os.environ["HSA_OVERRIDE_GFX_VERSION"] = "9.0.6"
os.environ["PYTORCH_HIP_ALLOC_CONF"] = "expandable_segments:True"


def main(adapter_path, test_data_path, output_file):
    # åŸºç¡€æ¨¡å‹è·¯å¾„
    BASE_MODEL_PATH = "/home/sleepy/Depot/ModelTrain/qwen/Qwen2___5-7B-Instruct"

    print(f"ğŸ”„ Processing Adapter: {adapter_path}")
    print(f"ğŸ“‚ Input Data: {test_data_path}")
    print(f"ğŸ’¾ Output File: {output_file}")

    # ================= 2. åŠ è½½æ•°æ®ä¸æ–­ç‚¹ç»­ä¼ æ£€æµ‹ =================
    with open(test_data_path, 'r', encoding='utf-8') as f:
        all_data = json.load(f)

    start_index = 0
    # æ£€æµ‹è¾“å‡ºæ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨åˆ™è®¡ç®—å·²è·‘è¡Œæ•°
    if os.path.exists(output_file):
        with open(output_file, 'r', encoding='utf-8') as f_out:
            # è®¡ç®—éç©ºè¡Œæ•°
            lines = [line for line in f_out if line.strip()]
            start_index = len(lines)
            if start_index > 0:
                print(f"âš ï¸  Found existing file with {start_index} samples. Resuming from index {start_index}...")

    # æˆªå–å‰©ä½™éœ€è¦è·‘çš„æ•°æ®
    data_to_process = all_data[start_index:]

    if len(data_to_process) == 0:
        print("ğŸ‰ All data processed! Nothing to do.")
        return

    # ================= 3. åŠ è½½æ¨¡å‹ (å¼ºåˆ¶ FP32) =================
    print("â³ Loading model into VRAM (FP32 Mode)...")
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_PATH,
        device_map="auto",
        torch_dtype=torch.float32,
        trust_remote_code=True
    )

    print(f"ğŸ”— Merging LoRA weights from {adapter_path}...")
    model = PeftModel.from_pretrained(model, adapter_path)
    model.eval()

    # ================= 4. æµå¼æ¨ç†ä¸å†™å…¥ =================
    print(f"ğŸš€ Starting inference on remaining {len(data_to_process)} samples...")

    # è‡ªåŠ¨åˆ›å»ºç›®å½•
    os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else ".", exist_ok=True)

    # ä½¿ç”¨ 'a' (append) æ¨¡å¼æ‰“å¼€æ–‡ä»¶
    # buffering=1 è¡¨ç¤ºè¡Œç¼“å†²ï¼Œflush() ä¼šæ›´æœ‰æ•ˆ
    with open(output_file, 'a', encoding='utf-8', buffering=1) as f:

        for item in tqdm(data_to_process, desc="Inference"):
            instruction = item.get("instruction", "")
            input_text = item.get("input", "")
            ground_truth = item.get("output", "")

            # æ„é€  Prompt
            messages = [
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": instruction + "\n" + input_text}
            ]

            text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

            try:
                with torch.no_grad():
                    generated_ids = model.generate(
                        **model_inputs,
                        max_new_tokens=512,
                        temperature=0.7,
                        top_p=0.9
                    )

                generated_ids = [
                    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
                ]
                response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
            except Exception as e:
                print(f"\nâŒ Error generating sample: {e}")
                response = "ERROR_GENERATION"

            result_entry = {
                "instruction": instruction,
                "input": input_text,
                "ground_truth": ground_truth,
                "model_output": response,
                "adapter": adapter_path
            }

            # ã€æ ¸å¿ƒä¿®æ”¹ã€‘ç«‹å³å†™å…¥å¹¶åˆ·æ–°
            # ensure_ascii=False ä¿è¯å†™å…¥çš„æ˜¯ä¸­æ–‡è€Œä¸æ˜¯ \uXXXX
            f.write(json.dumps(result_entry, ensure_ascii=False) + "\n")

            # å¼ºåˆ¶å°†ç¼“å†²åŒºå†…å®¹åˆ·å…¥ç¡¬ç›˜ï¼Œæ­¤æ—¶åˆ«äººæ‰“å¼€æ–‡ä»¶å°±èƒ½çœ‹åˆ°æœ€æ–°çš„ä¸€è¡Œ
            f.flush()
            # os.fsync(f.fileno()) # å¦‚æœä½ æ˜¯æç«¯æ‰ç”µææƒ§ç—‡ï¼Œå¯ä»¥å–æ¶ˆè¿™è¡Œæ³¨é‡Šï¼Œä½†ä¼šç¨å¾®æ…¢ä¸€ç‚¹ç‚¹

    print(f"âœ… Done! Results saved to {output_file}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--adapter", type=str, required=True, help="Path to checkpoint folder")
    parser.add_argument("--data", type=str, default="alpaca_test.json", help="Path to test json")
    parser.add_argument("--output", type=str, required=True, help="Path to output jsonl")

    args = parser.parse_args()
    main(args.adapter, args.data, args.output)
