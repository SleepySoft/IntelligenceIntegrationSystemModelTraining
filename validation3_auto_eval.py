import json
import re
import time
import random
import torch
import os
from typing import List, Dict, Any, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# ================= é…ç½®åŒºåŸŸ =================
# 1. æœ¬åœ°æ¨¡å‹é…ç½®
BASE_MODEL_PATH = "/home/sleepy/Depot/ModelTrain/qwen/Qwen2___5-7B-Instruct"
# æ›¿æ¢ä¸ºä½ æƒ³è¦è¯„ä¼°çš„é‚£ä¸ª Checkpoint è·¯å¾„
ADAPTER_PATH = "./saves/qwen2.5-7b-intelligence/lora/sft_ddp_fp32/checkpoint-xxx"

# 2. æ•°æ®é›†é…ç½®
TEST_DATA_PATH = "alpaca_test.json"  # æˆ–è€…æ˜¯ alpaca_val.json
OUTPUT_FILE = "evaluation_report.jsonl"

# 3. è¯„æµ‹é…ç½®
USE_REAL_MODEL = True  # True: è·‘çœŸå®æ¨¡å‹æ¨ç†; False: ä»…æµ‹è¯•è¯„åˆ†é€»è¾‘(ç”¨å‡æ•°æ®)
USE_REAL_API = False  # True: è°ƒç”¨çœŸå®API; False: ä½¿ç”¨Stubè¿”å›éšæœºåˆ†
NUM_WORKERS = 4  # API å¹¶å‘çº¿ç¨‹æ•°


# ================= Part 1: API å®¢æˆ·ç«¯å°è£… =================

class APIClient:
    def __init__(self, use_stub: bool = True):
        self.use_stub = use_stub

    def chat(self,
             messages: List[Dict[str, str]],
             model: Optional[str] = "gpt-4o",  # å‡è®¾è£åˆ¤æ˜¯ GPT-4o æˆ–ç±»ä¼¼å¼ºæ¨¡å‹
             temperature: float = 0.1,  # è¯„æµ‹æ—¶æ¸©åº¦è¦ä½ï¼Œä¿è¯ç¨³å®šæ€§
             max_tokens: int = 4096,
             is_health_check: bool = False) -> Dict[str, Any]:
        """
        ä½ çš„ API æ¥å£å®ç°
        """
        if self.use_stub:
            return self._stub_response(messages)

        # TODO: è¿™é‡Œå¡«å…¥ä½ çœŸå®çš„ API è°ƒç”¨é€»è¾‘ (requests / sdk)
        # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
        time.sleep(0.5)
        # è¿™æ˜¯ä¸€ä¸ª Mock çš„è¿”å›ç»“æ„ï¼Œéœ€æ ¹æ®ä½ å®é™… API è¿”å›ä¿®æ”¹
        return {
            "choices": [{
                "message": {
                    "content": self._mock_judge_logic()
                }
            }]
        }

    def _stub_response(self, messages) -> Dict[str, Any]:
        """æµ‹è¯•ç”¨çš„æ¡©"""
        time.sleep(0.1)
        return {
            "choices": [{
                "message": {
                    "content": self._mock_judge_logic()
                }
            }]
        }

    def _mock_judge_logic(self):
        """ç”Ÿæˆä¸€ä¸ªå‡çš„ JSON è¯„åˆ†è¿”å›"""
        score = random.randint(1, 10)
        reasoning = f"This is a stub evaluation. The model output length is fine. Random score: {score}."
        # æ¨¡æ‹Ÿ LLM æœ‰æ—¶ä¼šå¸¦ Markdown ä»£ç å—ï¼Œæœ‰æ—¶ç›´æ¥è¿”å› JSON
        json_str = json.dumps({"score": score, "reasoning": reasoning})
        return f"```json\n{json_str}\n```"


# ================= Part 2: è¯„æµ‹ Prompt æ¨¡æ¿ =================

JUDGE_PROMPT_TEMPLATE = """
### Task
You are an impartial and objective judge. You will be given an Instruction, an Input (optional), a Reference Answer (Ground Truth), and a Model Output.
Your task is to evaluate the quality of the 'Model Output' by comparing it to the 'Reference Answer' and the 'Instruction'.

### Scoring Criteria (1-10)
- **Accuracy**: Does the model answer the question correctly?
- **Completeness**: Does it cover all parts of the instruction?
- **Format**: Is the format correct (e.g., list, code, text)?
- **Hallucination**: Does the model invent false information?

### Input Data
**Instruction**: {instruction}
**Input**: {input}
**Reference Answer**: {ground_truth}
**Model Output**: {model_output}

### Output Format
You must return a strict JSON object with two fields:
1. "score": An integer from 1 to 10.
2. "reasoning": A concise explanation for the score.

Example output:
{{
    "score": 8,
    "reasoning": "The model answered correctly but missed one minor detail mentioned in the reference."
}}
"""


# ================= Part 3: æœ¬åœ°æ¨ç†å¼•æ“ =================

class InferenceEngine:
    def __init__(self, base_path, adapter_path):
        print(f"Loading local model from {adapter_path}...")
        self.tokenizer = AutoTokenizer.from_pretrained(base_path, trust_remote_code=True)
        # æ¨ç†æ—¶å¯ä»¥ç”¨ FP16ï¼Œæ˜¾å­˜å ç”¨å°ä¸”å¿«
        self.model = AutoModelForCausalLM.from_pretrained(
            base_path,
            device_map="auto",
            torch_dtype=torch.float16,
            trust_remote_code=True
        )
        self.model = PeftModel.from_pretrained(self.model, adapter_path)
        self.model.eval()
        print("Model loaded successfully.")

    def generate(self, instruction, input_text):
        messages = [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": f"{instruction}\n{input_text}"}
        ]
        text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        model_inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)

        with torch.no_grad():
            generated_ids = self.model.generate(
                **model_inputs,
                max_new_tokens=512,
                temperature=0.7,
                top_p=0.9
            )
            # è£å‰ªæ‰ Input éƒ¨åˆ†ï¼Œåªç•™ Output
            generated_ids = [
                output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
            ]
            response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        return response


# ================= Part 4: ä¸»æµç¨‹ (ç”Ÿæˆ + è‡ªåŠ¨è¯„åˆ†) =================

def parse_judge_response(response_content: str) -> Dict:
    """è§£æ API è¿”å›çš„ JSONï¼Œå¤„ç†å¯èƒ½çš„ Markdown æ ¼å¼"""
    try:
        # ç§»é™¤ ```json å’Œ ``` æ ‡è®°
        content = re.sub(r'```json\s*', '', response_content)
        content = re.sub(r'```\s*', '', content)
        content = content.strip()
        return json.loads(content)
    except Exception as e:
        return {"score": -1, "reasoning": f"Parse Error: {str(e)} | Raw: {response_content}"}


def run_evaluation():
    # 1. åˆå§‹åŒ–
    client = APIClient(use_stub=not USE_REAL_API)

    if USE_REAL_MODEL:
        engine = InferenceEngine(BASE_MODEL_PATH, ADAPTER_PATH)

    # 2. è¯»å–æµ‹è¯•é›†
    if not os.path.exists(TEST_DATA_PATH):
        print(f"Test data not found: {TEST_DATA_PATH}, creating dummy data.")
        # åˆ›å»ºå‡æ•°æ®ç”¨äºæµ‹è¯•è„šæœ¬æµç¨‹
        test_data = [
                        {"instruction": "Calculate 1+1", "input": "", "output": "The answer is 2."}
                    ] * 5
    else:
        with open(TEST_DATA_PATH, 'r', encoding='utf-8') as f:
            test_data = json.load(f)
            # ä¸ºäº†æµ‹è¯•æ–¹ä¾¿ï¼Œå…ˆåªå–å‰ 10 æ¡ï¼Œå®é™…è·‘æ—¶å»æ‰åˆ‡ç‰‡
            # test_data = test_data[:10]

    results = []

    print(f"ğŸš€ Starting Auto-Evaluation on {len(test_data)} samples...")
    print(f"Configuration: Real_Model={USE_REAL_MODEL}, Real_API={USE_REAL_API}, Workers={NUM_WORKERS}")

    # 3. æ­¥éª¤ä¸€ï¼šæœ¬åœ°æ¨ç† (Serial, GPU bound)
    # å¦‚æœå·²ç»æœ‰æ¨ç†ç»“æœæ–‡ä»¶ï¼Œå¯ä»¥è·³è¿‡è¿™ä¸€æ­¥ç›´æ¥è¯»å–
    inference_results = []
    for item in tqdm(test_data, desc="Local Inference"):
        if USE_REAL_MODEL:
            model_output = engine.generate(item.get("instruction", ""), item.get("input", ""))
        else:
            model_output = "Dummy model output for testing."

        item['model_output'] = model_output
        inference_results.append(item)

    # 4. æ­¥éª¤äºŒï¼šAPI è¯„åˆ† (Parallel, IO bound)
    print("âš–ï¸  Submitting to Judge API...")

    with ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:
        future_to_item = {}
        for item in inference_results:
            # æ„é€  Prompt
            prompt = JUDGE_PROMPT_TEMPLATE.format(
                instruction=item.get("instruction", ""),
                input=item.get("input", ""),
                ground_truth=item.get("output", ""),
                model_output=item.get("model_output", "")
            )

            messages = [{"role": "system", "content": "You are a helpful assistant."},
                        {"role": "user", "content": prompt}]

            # æäº¤ä»»åŠ¡
            future = executor.submit(client.chat, messages=messages)
            future_to_item[future] = item

        # è·å–ç»“æœ
        completed_count = 0
        total_score = 0
        valid_scores = 0

        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f_out:
            for future in tqdm(as_completed(future_to_item), total=len(inference_results), desc="Judging"):
                item = future_to_item[future]
                try:
                    # è·å– API åŸå§‹è¿”å›
                    api_resp = future.result()
                    # å‡è®¾ä½ çš„ API è¿”å›ç»“æ„æ˜¯æ ‡å‡†çš„ OpenAI æ ¼å¼
                    content = api_resp['choices'][0]['message']['content']

                    # è§£æ JSON
                    judge_result = parse_judge_response(content)

                    item['judge_score'] = judge_result.get('score', -1)
                    item['judge_reasoning'] = judge_result.get('reasoning', 'No reasoning')

                    # ç»Ÿè®¡
                    if item['judge_score'] != -1:
                        total_score += item['judge_score']
                        valid_scores += 1

                    # å†™å…¥æ–‡ä»¶
                    f_out.write(json.dumps(item, ensure_ascii=False) + "\n")
                    f_out.flush()

                except Exception as e:
                    print(f"Error processing item: {e}")

    # 5. æ€»ç»“æŠ¥å‘Š
    avg_score = total_score / valid_scores if valid_scores > 0 else 0
    print("\n" + "=" * 30)
    print(f"ğŸ“Š Evaluation Complete!")
    print(f"Output saved to: {OUTPUT_FILE}")
    print(f"Total Samples: {len(inference_results)}")
    print(f"Valid Evaluations: {valid_scores}")
    print(f"ğŸ† Average Score: {avg_score:.2f} / 10.0")
    print("=" * 30)


if __name__ == "__main__":
    run_evaluation()