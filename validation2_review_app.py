import streamlit as st
import json
import os
import re
import pandas as pd
from collections import Counter

# ================= é…ç½® =================
# DATA_FILE = "Data/v1/result_ckpt100.jsonl"
DATA_FILE = "evaluation-20260103/result_ckpt360.jsonl"
REVIEWED_FILE = "eval_reviewed.jsonl"

st.set_page_config(layout="wide", page_title="Model Evaluation Tool - Advanced")


# --- 1. æ ¸å¿ƒè§£æžä¸Žè¯„ä¼°é€»è¾‘ (Core Logic) ---

def safe_parse_json(text):
    """
    å°è¯•è§£æž JSONï¼Œå¦‚æžœå¤±è´¥è¿”å›ž Noneã€‚
    """
    if text is None:
        return None
    if isinstance(text, dict):
        return text

    text = str(text).strip()
    try:
        return json.loads(text)
    except:
        pass

    # å°è¯•æå– markdown json
    match = re.search(r'```json\s*(.*?)\s*```', text, re.DOTALL)
    if match:
        try:
            return json.loads(match.group(1))
        except:
            pass

    # å°è¯•æå–ç¬¬ä¸€ä¸ª {}
    match = re.search(r'(\{.*\})', text, re.DOTALL)
    if match:
        try:
            return json.loads(match.group(1))
        except:
            pass

    return None


def is_negative_sample(data_dict):
    """
    åˆ¤æ–­æ˜¯å¦ä¸ºè´Ÿä¾‹ï¼ˆæ— ä»·å€¼æƒ…æŠ¥ï¼‰ã€‚
    é€»è¾‘ï¼šå¦‚æžœæ˜¯ Noneï¼Œè§†ä¸ºé”™è¯¯ï¼ˆä¸æ˜¯è´Ÿä¾‹ï¼‰ã€‚
    å¦‚æžœ Key åªæœ‰ UUIDï¼Œæˆ–è€…æ²¡æœ‰ RATE/EVENT_TEXT å­—æ®µï¼Œè§†ä¸ºè´Ÿä¾‹ã€‚
    """
    if not data_dict or not isinstance(data_dict, dict):
        return False

    # é€»è¾‘ï¼šåªæœ‰ UUID æˆ– æ˜¾å¼ä¸ºç©º
    keys = set(data_dict.keys())
    if keys == {'UUID'}:
        return True

    # æˆ–è€…æ²¡æœ‰æ ¸å¿ƒå†…å®¹å­—æ®µ
    if 'RATE' not in data_dict and 'EVENT_TEXT' not in data_dict:
        return True

    return False


def extract_scores(rate_data):
    """
    è§£æž RATE å­—å…¸ã€‚
    è¿”å›ž:
    1. independent_scores: { 'å†…å®¹å‡†ç¡®çŽ‡': val, 'è§„æ¨¡åŠå½±å“': val, 'æ½œåŠ›åŠä¼ æ‰¿': val }
    2. primary_category: (Name, Score) - é™¤åŽ»ä¸Šè¿°ä¸‰ä¸ªkeyåŽçš„æœ€é«˜åˆ†é¡¹
    """
    if not isinstance(rate_data, dict):
        return {}, ("N/A", 0)

    independent_keys = {"å†…å®¹å‡†ç¡®çŽ‡", "è§„æ¨¡åŠå½±å“", "æ½œåŠ›åŠä¼ æ‰¿"}

    # æå–ç‹¬ç«‹åˆ†æ•°
    independent_scores = {k: rate_data.get(k, 0) for k in independent_keys}

    # æå–ä¸»è¦ç»´åº¦
    candidates = {k: v for k, v in rate_data.items() if k not in independent_keys}

    if not candidates:
        return independent_scores, ("æ— æœ‰æ•ˆé¢†åŸŸ", 0)

    best_category = max(candidates, key=candidates.get)
    best_score = candidates[best_category]

    return independent_scores, (best_category, best_score)


def evaluate_single_sample(gt_raw, pred_raw):
    """
    å¯¹å•æ¡æ•°æ®è¿›è¡Œè‡ªåŠ¨åŒ–è¯„ä¼°ï¼Œè¿”å›žè¯„ä¼°ç»“æžœå¯¹è±¡
    """
    gt = safe_parse_json(gt_raw)
    pred = safe_parse_json(pred_raw)

    result = {
        "format_error": False,
        "uuid_missing": False,
        "classification": "Unknown",  # TP, TN, FP, FN
        "dim_match": None,  # ä¸»è¦ç»´åº¦æ˜¯å¦ä¸€è‡´
        "score_deltas": {},  # ç‹¬ç«‹è¯„åˆ†åå·®
        "details": ""
    }

    # 1. æ£€æŸ¥æ ¼å¼é”™è¯¯
    if pred is None:
        result["format_error"] = True
        return result

    if "UUID" not in pred:
        result["uuid_missing"] = True
        result["format_error"] = True  # è§†ä½œæ ¼å¼é”™è¯¯
        return result

    # 2. æ£€æŸ¥æ­£è´Ÿä¾‹
    gt_is_neg = is_negative_sample(gt)
    pred_is_neg = is_negative_sample(pred)

    if not gt_is_neg and not pred_is_neg:
        result["classification"] = "TP"  # éƒ½æœ‰å†…å®¹
    elif gt_is_neg and pred_is_neg:
        result["classification"] = "TN"  # éƒ½è®¤ä¸ºæ²¡å†…å®¹
    elif gt_is_neg and not pred_is_neg:
        result["classification"] = "FP"  # GTæ— å†…å®¹ï¼Œæ¨¡åž‹ç¼–é€ äº†å†…å®¹
    elif not gt_is_neg and pred_is_neg:
        result["classification"] = "FN"  # GTæœ‰å†…å®¹ï¼Œæ¨¡åž‹å¿½ç•¥äº†

    # 3. å¦‚æžœæ˜¯ TP (ä¸¤è€…éƒ½æœ‰å†…å®¹)ï¼Œæ·±å…¥å¯¹æ¯”ç»´åº¦å’Œåˆ†æ•°
    if result["classification"] == "TP":
        gt_indep, (gt_cat, _) = extract_scores(gt.get("RATE", {}))
        pred_indep, (pred_cat, _) = extract_scores(pred.get("RATE", {}))

        # ç»´åº¦å¯¹æ¯”
        result["dim_match"] = (gt_cat == pred_cat)
        result["gt_primary"] = gt_cat
        result["pred_primary"] = pred_cat

        # åˆ†æ•°å¯¹æ¯” (Pred - GT)
        for k in gt_indep:
            result["score_deltas"][k] = pred_indep.get(k, 0) - gt_indep[k]

    return result


def calculate_global_metrics(data_list):
    """
    éåŽ†æ‰€æœ‰æ•°æ®ï¼Œè®¡ç®—å…¨å±€æŒ‡æ ‡
    """
    stats = {
        "total": len(data_list),
        "format_errors": 0,
        "TP": 0, "TN": 0, "FP": 0, "FN": 0,
        "dim_match_count": 0,
        "tp_count_for_dim": 0,
        "score_mae": {"å†…å®¹å‡†ç¡®çŽ‡": [], "è§„æ¨¡åŠå½±å“": [], "æ½œåŠ›åŠä¼ æ‰¿": []}
    }

    for item in data_list:
        eval_res = evaluate_single_sample(item.get('ground_truth'), item.get('model_output'))

        if eval_res["format_error"]:
            stats["format_errors"] += 1
            continue  # æ ¼å¼é”™è¯¯ä¸å‚ä¸ŽåŽç»­é€»è¾‘æ··æ·†çŸ©é˜µè®¡ç®—

        cls = eval_res["classification"]
        stats[cls] += 1

        if cls == "TP":
            stats["tp_count_for_dim"] += 1
            if eval_res["dim_match"]:
                stats["dim_match_count"] += 1

            for k, delta in eval_res["score_deltas"].items():
                if k in stats["score_mae"]:
                    stats["score_mae"][k].append(abs(delta))

    return stats


# --- 2. Helper Functions (File IO) ---
def load_data():
    data = []
    if os.path.exists(DATA_FILE):
        with open(DATA_FILE, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    data.append(json.loads(line))
    return data


def save_progress(index, label, comment, current_data):
    current_data[index]['human_label'] = label
    current_data[index]['comments'] = comment
    with open(DATA_FILE, 'w', encoding='utf-8') as f:
        for entry in current_data:
            f.write(json.dumps(entry, ensure_ascii=False) + "\n")


# --- 3. UI æ¸²æŸ“å‡½æ•° ---

def render_metrics_sidebar(data):
    st.sidebar.title("ðŸ“Š Auto Evaluation Stats")

    if not data:
        st.sidebar.warning("No Data Loaded")
        return

    stats = calculate_global_metrics(data)
    total_valid = stats["TP"] + stats["TN"] + stats["FP"] + stats["FN"]

    # 1. æ ¼å¼é”™è¯¯çŽ‡
    err_rate = (stats["format_errors"] / stats["total"]) * 100 if stats["total"] > 0 else 0
    st.sidebar.metric("JSON Format Error Rate", f"{err_rate:.1f}%", help="æ— æ³•è§£æžJSONæˆ–ç¼ºå°‘UUIDçš„æ¯”ä¾‹")

    st.sidebar.divider()

    # 2. æ··æ·†çŸ©é˜µæŒ‡æ ‡
    # Precision = TP / (TP + FP)
    precision = stats["TP"] / (stats["TP"] + stats["FP"]) if (stats["TP"] + stats["FP"]) > 0 else 0
    # Recall = TP / (TP + FN)
    recall = stats["TP"] / (stats["TP"] + stats["FN"]) if (stats["TP"] + stats["FN"]) > 0 else 0
    # Accuracy = (TP + TN) / Total Valid
    acc = (stats["TP"] + stats["TN"]) / total_valid if total_valid > 0 else 0

    c1, c2 = st.sidebar.columns(2)
    c1.metric("Precision", f"{precision:.2%}")
    c2.metric("Recall", f"{recall:.2%}")
    st.sidebar.metric("Classification Acc", f"{acc:.2%}", help="æ­£ç¡®åˆ¤æ–­ 'æœ‰ä»·å€¼' vs 'æ— ä»·å€¼' çš„å‡†ç¡®çŽ‡")

    st.sidebar.text(f"TP:{stats['TP']} | TN:{stats['TN']} | FP:{stats['FP']} | FN:{stats['FN']}")

    st.sidebar.divider()

    # 3. ç»´åº¦ä¸Žè¯„åˆ†
    dim_acc = stats["dim_match_count"] / stats["tp_count_for_dim"] if stats["tp_count_for_dim"] > 0 else 0
    st.sidebar.metric("Primary Dimension Match", f"{dim_acc:.1f}%", help="åœ¨åŒæ–¹éƒ½è®¤ä¸ºæœ‰ä»·å€¼æ—¶ï¼Œä¸»è¦åˆ†ç±»ç»´åº¦çš„ä¸€è‡´æ€§")

    st.sidebar.write("Score MAE (å¹³å‡ç»å¯¹è¯¯å·®):")
    for k, v_list in stats["score_mae"].items():
        avg_mae = sum(v_list) / len(v_list) if v_list else 0
        st.sidebar.caption(f"{k}: {avg_mae:.2f}")


def render_content_card(column, title, raw_data, style="default", compare_eval=None, is_gt=False):
    """
    Enhanced render function based on evaluation results.
    """
    data_dict = safe_parse_json(raw_data)

    with column:
        # æ ‡é¢˜è¡Œ
        header_cols = st.columns([3, 1])
        header_cols[0].markdown(f"### {title}")

        # å¦‚æžœæ˜¯æ¨¡åž‹è¾“å‡ºä¸”æœ‰é”™è¯¯ï¼Œæ˜¾ç¤ºåœ¨è¿™é‡Œ
        if not is_gt and compare_eval:
            if compare_eval["format_error"]:
                header_cols[1].error("FORMAT ERR")
            elif compare_eval["classification"] == "FN":
                header_cols[1].error("MISSED (FN)")
            elif compare_eval["classification"] == "FP":
                header_cols[1].warning("NOISE (FP)")
            elif compare_eval["classification"] == "TN":
                header_cols[1].info("IGNORE (TN)")

        if data_dict is None:
            st.error("âš ï¸ JSON Parse Error")
            st.code(str(raw_data), language="text")
            return

        # åˆ¤æ–­æ˜¯å¦ä¸ºè´Ÿä¾‹ (ä»… UUID)
        is_neg = is_negative_sample(data_dict)

        if is_neg:
            st.info(f"ðŸš« Negative Sample (No Value)\nUUID: {data_dict.get('UUID', 'Unknown')}")
        else:
            # æ­£å¸¸å†…å®¹å±•ç¤º
            indep_scores, (prim_cat, prim_score) = extract_scores(data_dict.get("RATE", {}))

            # é¢œè‰²é€»è¾‘ï¼šå¦‚æžœç»´åº¦ä¸åŒ¹é…ï¼Œä¸”å½“å‰æ˜¯æ¨¡åž‹è¾“å‡ºï¼Œä¸”ä¸æ˜¯GTï¼Œæ˜¾ç¤ºé†’ç›®é¢œè‰²
            cat_delta = None
            if not is_gt and compare_eval and compare_eval["classification"] == "TP":
                if not compare_eval["dim_match"]:
                    cat_delta = "MISMATCH"

            # æŒ‡æ ‡å±•ç¤º
            m1, m2, m3, m4 = st.columns(4)
            m1.metric(label="ä¸»è¦ç»´åº¦", value=prim_cat, delta=cat_delta, delta_color="inverse")
            m2.metric(label="ä¸»åˆ†", value=prim_score)
            m3.metric(label="è§„æ¨¡å½±å“", value=indep_scores.get("è§„æ¨¡åŠå½±å“", 0))
            m4.metric(label="å†…å®¹å‡†ç¡®", value=indep_scores.get("å†…å®¹å‡†ç¡®çŽ‡", 0))

            st.divider()

            display_text = data_dict.get("EVENT_TEXT", str(raw_data))
            if style == "success":
                st.success(display_text)
            elif style == "warning":
                st.warning(display_text)
            else:
                st.info(display_text)

        with st.expander("æŸ¥çœ‹åŽŸå§‹ JSON"):
            st.json(data_dict)


# --- Main App Logic ---
def main():
    st.title("ðŸ¤– LLM Evaluation: Auto-Metrics & Human Review")

    # 1. åˆå§‹åŒ–
    if 'data' not in st.session_state:
        st.session_state.data = load_data()

    if 'current_index' not in st.session_state:
        unreviewed_indices = [i for i, d in enumerate(st.session_state.data) if d.get('human_label') is None]
        st.session_state.current_index = unreviewed_indices[0] if unreviewed_indices else 0

    data = st.session_state.data

    # --- æ¸²æŸ“ä¾§è¾¹æ ç»Ÿè®¡ ---
    render_metrics_sidebar(data)

    # --- ä¸»ç•Œé¢ ---
    idx = st.session_state.current_index
    total_count = len(data)

    # é¡¶éƒ¨è¿›åº¦
    reviewed_count = sum(1 for d in data if d.get('human_label') is not None)
    st.progress(reviewed_count / total_count if total_count > 0 else 0)

    if idx < total_count:
        item = data[idx]

        # å®žæ—¶è®¡ç®—å½“å‰æ¡ç›®çš„è‡ªåŠ¨è¯„ä¼°ç»“æžœ
        eval_result = evaluate_single_sample(item.get('ground_truth'), item.get('model_output'))

        st.subheader(f"Sample #{idx + 1} | Auto-Eval: {eval_result['classification']}")

        # å¯¹æ¯”åŒº
        col1, col2 = st.columns(2)

        # Ground Truth
        render_content_card(
            column=col1,
            title="âœ… Ground Truth",
            raw_data=item.get('ground_truth', '{}'),
            style="success",
            is_gt=True
        )

        # Model Output
        render_content_card(
            column=col2,
            title="ðŸ¤– Model Output",
            raw_data=item.get('model_output', '{}'),
            style="warning",
            compare_eval=eval_result,  # ä¼ å…¥è¯„ä¼°ç»“æžœç”¨äºŽé«˜äº®å·®å¼‚
            is_gt=False
        )

        # --- è¯¦ç»†å¯¹æ¯”ä¿¡æ¯ (å¦‚æžœå‡ºé”™æˆ–ä¸ä¸€è‡´) ---
        if eval_result["format_error"]:
            st.error(f"âŒ Critical Error: Model output format is invalid or missing UUID.")
        elif eval_result["classification"] == "TP" and not eval_result["dim_match"]:
            st.warning(
                f"âš ï¸ Dimension Mismatch: GT implies '{eval_result['gt_primary']}' but Model predicts '{eval_result['pred_primary']}'")
        elif eval_result["classification"] == "FN":
            st.error("âš ï¸ Recall Failure: Ground truth has valid info, model returned Negative.")
        elif eval_result["classification"] == "FP":
            st.warning("âš ï¸ Precision Failure: Ground truth is Negative, model hallucinated info.")

        # --- æ“ä½œåŒº ---
        st.divider()
        c1, c2, c3 = st.columns([1, 1, 4])

        with c1:
            if st.button("ðŸ‘ Pass / Good", use_container_width=True, type="primary"):
                save_progress(idx, "pass", "", data)
                st.session_state.current_index += 1
                st.rerun()

        with c2:
            if st.button("ðŸ‘Ž Fail / Bad", use_container_width=True):
                save_progress(idx, "fail", "", data)
                st.session_state.current_index += 1
                st.rerun()

        with c3:
            comment = st.text_input("Comments", key="comment_input", placeholder="e.g. Logic error, Wrong score...")
            if st.button("Submit Comment"):
                save_progress(idx, "commented", comment, data)
                st.session_state.current_index += 1
                st.rerun()

        # å¯¼èˆª
        st.divider()
        prev, center, next_btn = st.columns([1, 8, 1])
        if prev.button("Previous"):
            st.session_state.current_index = max(0, idx - 1)
            st.rerun()
        if next_btn.button("Next"):
            st.session_state.current_index = min(len(data) - 1, idx + 1)
            st.rerun()

        with st.expander("Show Instruction & Input"):
            st.info(f"**Instruction:** {item.get('instruction', '')}")
            st.text(f"**Input:** {item.get('input', '')}")

    else:
        st.balloons()
        st.success("ðŸŽ‰ All samples reviewed!")

        # æœ€ç»ˆä¸‹è½½
        st.download_button(
            label="Download Reviewed Data",
            data=json.dumps(data, indent=2, ensure_ascii=False),
            file_name="reviewed_final.json",
            mime="application/json"
        )


if __name__ == "__main__":
    main()
